# Project Journey: Chandamama Magazine Metadata Extraction

**Date:** December 24, 2025
**Project:** Chandamama 2.0 Catalogue Indexing

## 1. Executive Summary
This report documents the development of an automated pipeline to extract, index, and analyze metadata from the *Chandamama* magazine archive (1947â€“2012). The goal was to transform thousands of scanned PDF files into a structured, searchable dataset containing story titles, authors, genres, and keywords.

## 2. Methodology & Evolution
The project evolved through several phases to address technical challenges with large-scale processing.

### Phase 1: REST API Prototype
*   **Approach**: Initial implementation used the Gemini API via raw HTTP requests (`requests` library).
*   **Method**: PDFs were encoded as Base64 strings and embedded directly in the JSON payload.
*   **Challenge**: This method proved fragile for large magazine PDFs (>10MB) or issues with older years (e.g., 1954), resulting in timeouts and `API Error (No Response)`.

### Phase 2: Robust SDK Integration (File API)
*   **Solution**: Refactored the engine to use the official `google-generativeai` Python SDK.
*   **Key Change**: Implemented the **File API** (`genai.upload_file`).
*   **Benefit**: Instead of sending Base64 payloads, files are uploaded to Google's temporary storage first. The model then processes the file handle, supporting files up to 2GB and eliminating payload size errors.
*   **Outcome**: Significantly improved success rate for batch processing.

### Phase 3: Analytics & Statistics
*   **Goal**: Aggregate metadata to understand the corpus.
*   **Metrics**:
    *   **Authors**: tracked unique identifiers for contributors.
    *   **Genres**: categorized stories (Mythology, Fable, etc.).
    *   **Keywords**: implemented extraction of semantic tags for searchability.
*   **Implementation**: A global statistics aggregator scans the entire output catalogue to maintain live counts.

## 3. Technical Architecture

### Core Indexer (`streamlit_indexer.py`)
The main application consists of a Streamlit frontend and a Python backend.

#### Features
*   **Dual Mode**: Single File debugging and Bulk Directory processing.
*   **Automatic Backup**: `_bulk_progress_backup.json` saves state after every file to prevent data loss.
*   **Auto-Repair**: Integrated `json_repair` to fix malformed JSON output from the LLM.
*   **Safety Handling**: Tuned safety settings (including `CIVIC_INTEGRITY`) to preventing false-positive blocking.

### Statistics Regeneration
To ensure `global_stats.json` remains accurate even after manual edits or re-runs, we developed a regeneration logic.

```python
# Key Logic for Keyword Aggregation
keywords = story.get('keywords', [])
if isinstance(keywords, list):
    for kw in keywords:
        keyword_counts[kw.strip()] += 1
```

## 4. Current Status
*   **Total Stories Indexed**: ~10,200+
*   **Unique Authors**: ~2,500+
*   **Keywords Tracked**: Yes (Global Count Enabled)

## 5. Future Roadmap
*   **Search Interface**: Build a frontend to search stories by keyword/author.
*   **Visual Analytics**: Visualize the rise and fall of genres over the decades.
*   **Vector Search**: Embed story content for semantic search capabilities.

---
*Generated by Antigravity Agent*
